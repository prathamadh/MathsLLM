{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets","metadata":{"execution":{"iopub.status.busy":"2024-09-28T07:51:50.259791Z","iopub.execute_input":"2024-09-28T07:51:50.260939Z","iopub.status.idle":"2024-09-28T07:52:03.280196Z","shell.execute_reply.started":"2024-09-28T07:51:50.260875Z","shell.execute_reply":"2024-09-28T07:52:03.278784Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"MARIO-Math-Reasoning/AlphaMath-Trainset\")","metadata":{"execution":{"iopub.status.busy":"2024-09-28T07:52:03.282435Z","iopub.execute_input":"2024-09-28T07:52:03.282862Z","iopub.status.idle":"2024-09-28T07:52:20.645011Z","shell.execute_reply.started":"2024-09-28T07:52:03.282801Z","shell.execute_reply":"2024-09-28T07:52:20.643991Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/822 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"315566788a8a4289b6bf69b219f62020"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"round3_training_data.json:   0%|          | 0.00/215M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbd27ee20a214394b8904871138db0a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/115551 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60687ce6422b46c58271f44184bfca4e"}},"metadata":{}}]},{"cell_type":"code","source":"print(ds)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:10:13.300988Z","iopub.execute_input":"2024-09-26T10:10:13.301518Z","iopub.status.idle":"2024-09-26T10:10:13.308543Z","shell.execute_reply.started":"2024-09-26T10:10:13.301462Z","shell.execute_reply":"2024-09-26T10:10:13.307148Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['answer', 'id', 'output', 'instruction', 'input'],\n        num_rows: 115551\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd \ntrain_df=pd.DataFrame(ds[\"train\"])\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:10:13.311183Z","iopub.execute_input":"2024-09-26T10:10:13.311638Z","iopub.status.idle":"2024-09-26T10:10:20.018813Z","shell.execute_reply.started":"2024-09-26T10:10:13.311596Z","shell.execute_reply":"2024-09-26T10:10:20.017401Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:10:20.020516Z","iopub.execute_input":"2024-09-26T10:10:20.020912Z","iopub.status.idle":"2024-09-26T10:10:20.039699Z","shell.execute_reply.started":"2024-09-26T10:10:20.020869Z","shell.execute_reply":"2024-09-26T10:10:20.038443Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"  answer         id                                             output  \\\n0    234  gsm8k_525  [{\"step\": \"<step>\\n<p>\\nTo find how many books...   \n1    234  gsm8k_525  [{\"step\": \"<step>\\n<p>\\nTo find how many books...   \n2    234  gsm8k_525  [{\"step\": \"<step>\\n<p>\\nTo calculate how many ...   \n3    234  gsm8k_525  [{\"step\": \"<step>\\n<p>\\nTo find the number of ...   \n4    234  gsm8k_525  [{\"step\": \"<step>\\n<p>\\nTo solve this problem,...   \n\n                                         instruction input  \n0  <question> There are 336 books in a library. O...        \n1  <question> There are 336 books in a library. O...        \n2  <question> There are 336 books in a library. O...        \n3  <question> There are 336 books in a library. O...        \n4  <question> There are 336 books in a library. O...        ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>answer</th>\n      <th>id</th>\n      <th>output</th>\n      <th>instruction</th>\n      <th>input</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>234</td>\n      <td>gsm8k_525</td>\n      <td>[{\"step\": \"&lt;step&gt;\\n&lt;p&gt;\\nTo find how many books...</td>\n      <td>&lt;question&gt; There are 336 books in a library. O...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>234</td>\n      <td>gsm8k_525</td>\n      <td>[{\"step\": \"&lt;step&gt;\\n&lt;p&gt;\\nTo find how many books...</td>\n      <td>&lt;question&gt; There are 336 books in a library. O...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>234</td>\n      <td>gsm8k_525</td>\n      <td>[{\"step\": \"&lt;step&gt;\\n&lt;p&gt;\\nTo calculate how many ...</td>\n      <td>&lt;question&gt; There are 336 books in a library. O...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>234</td>\n      <td>gsm8k_525</td>\n      <td>[{\"step\": \"&lt;step&gt;\\n&lt;p&gt;\\nTo find the number of ...</td>\n      <td>&lt;question&gt; There are 336 books in a library. O...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>234</td>\n      <td>gsm8k_525</td>\n      <td>[{\"step\": \"&lt;step&gt;\\n&lt;p&gt;\\nTo solve this problem,...</td>\n      <td>&lt;question&gt; There are 336 books in a library. O...</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nimport json\n\n# Sample DataFrame containing the column with the string\n\n# Function to parse JSON-like strings into columns\ndef parse_json_column(json_str):\n    # Parse the string into a Python list of dictionaries\n    json_data = json.loads(json_str)\n    \n    # Convert the list of dictionaries into a DataFrame\n    return pd.DataFrame(json_data)\n\n# Apply the function to each row of the DataFrame and expand into separate columns\nexpanded_df = train_df['output'].apply(parse_json_column)\n\n# Concatenate the list of DataFrames\nfinal_df = pd.concat(expanded_df.tolist(), ignore_index=True)\n\n# Show the final DataFrame with separated columns\nprint(final_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:10:20.041190Z","iopub.execute_input":"2024-09-26T10:10:20.041705Z","iopub.status.idle":"2024-09-26T10:11:06.263012Z","shell.execute_reply.started":"2024-09-26T10:10:20.041652Z","shell.execute_reply":"2024-09-26T10:11:06.261864Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"                                                     step         P         Q  \\\n0       <step>\\n<p>\\nTo find how many books are there ...  0.997700  0.991211   \n1       <step>\\n<p>\\nAfter performing the calculation,...  0.950430  1.000000   \n2       <step>\\n<p>\\nTo find how many books are there ...  0.997700  0.991211   \n3       <step>\\n<p>\\nI have calculated the number of b...  0.998441  1.000000   \n4       <step>\\n<p>\\nTo calculate how many books there...  0.972948  0.988281   \n...                                                   ...       ...       ...   \n309322  <step>\\n<p>\\nIt seems that the code is not wor...  0.999806 -0.878906   \n309323  <step>\\n<p>\\nIt seems that the code is not wor...  0.935737 -1.000000   \n309324  <step>\\n<p>\\nTo find the sum of the $2007$ roo...  0.932510 -0.795796   \n309325  <step>\\n<p>\\nThe `sum` function is not availab...  0.975516 -0.976128   \n309326  <step>\\n<p>\\nFrom the result, we can see that ...  0.989254 -1.000000   \n\n        depth  \n0           1  \n1           2  \n2           1  \n3           2  \n4           1  \n...       ...  \n309322      3  \n309323      4  \n309324      1  \n309325      2  \n309326      3  \n\n[309327 rows x 4 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_train_df=final_df[1:50000]\nto_test_df=final_df[51000:52000]","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:11:06.264389Z","iopub.execute_input":"2024-09-26T10:11:06.264851Z","iopub.status.idle":"2024-09-26T10:11:06.270514Z","shell.execute_reply.started":"2024-09-26T10:11:06.264795Z","shell.execute_reply":"2024-09-26T10:11:06.269278Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"to_train_df.info()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:11:06.272406Z","iopub.execute_input":"2024-09-26T10:11:06.272903Z","iopub.status.idle":"2024-09-26T10:11:06.303856Z","shell.execute_reply.started":"2024-09-26T10:11:06.272852Z","shell.execute_reply":"2024-09-26T10:11:06.302704Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 49999 entries, 1 to 49999\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   step    49999 non-null  object \n 1   P       49999 non-null  float64\n 2   Q       49999 non-null  float64\n 3   depth   49999 non-null  int64  \ndtypes: float64(2), int64(1), object(1)\nmemory usage: 1.5+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import BartForSequenceClassification, BartTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\nfrom datasets import Dataset\nimport pandas as pd\n\n# Load your dataset (ensure that 'final_df' has the necessary columns like 'step' and 'P')\nto_train_df # Assuming final_df is already loaded and processed\n\n# Convert pandas DataFrame to Hugging Face Dataset\ndataset = Dataset.from_pandas(to_train_df)\neval_dataset=Dataset.from_pandas(to_test_df)\n\n# Load the BART tokenizer\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n\n# Tokenize the input texts\ndef tokenize_function(example):\n    return tokenizer(example['step'], padding='max_length', truncation=True, max_length=1024)\n\n# Apply tokenization with padding and truncation\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\neval_dataset=eval_dataset.map(tokenize_function, batched=True)\n# Load the pre-trained BART model and modify for regression\nclass BartForRegression(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bart = BartForSequenceClassification.from_pretrained('facebook/bart-base', num_labels=1)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        outputs = self.bart(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        loss = None\n        if labels is not None:\n            loss_fct = torch.nn.MSELoss()\n            loss = loss_fct(logits.view(-1), labels.view(-1))\n        return {'loss': loss, 'logits': logits}\n\n# Instantiate the model\nmodel = BartForRegression()\n\n# Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,              # Adjust this as needed\n    per_device_train_batch_size=4,   # Adjust batch size as needed\n    evaluation_strategy=\"epoch\",     # Evaluate after each epoch\n    save_strategy=\"epoch\",\n    logging_dir='./logs',\n    logging_steps=1000,\n)\n\n# Convert the 'P' column to a float tensor\ndef add_labels(example):\n    example['labels'] = torch.tensor(example['P'], dtype=torch.float)\n    return example\n\n# Apply the label transformation\ntokenized_dataset = tokenized_dataset.map(add_labels)\neval_dataset=eval_dataset.map(add_labels)\nprint(\"Tokenized input length: \", len(tokenized_dataset[0]['input_ids']))\n\n\n\n# Remove unnecessary columns that the model doesn't need for training\ntokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\neval_dataset=eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n# Use DataCollatorWithPadding to handle padding dynamically\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Create a Trainer object\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=data_collator  # Ensure proper padding\n)\n\n# Train the model\ntrainer.train()\n\n# Save the trained model\ntrainer.save_model(\"./trained_bart_regression\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:11:06.305611Z","iopub.execute_input":"2024-09-26T10:11:06.305988Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fcd884707c941b19062c724826b9867"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05b7bdb851b2413b8b6790d4d30f3d24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffdd70a178ab476a974108233b5c1911"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34d213ada8b8497785a6b3b703f46fbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3afe6be1acf4ddfb52f3d5c0dec4673"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/49999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af6edf569598415bba9cec0999f15d7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4e1ed7d64cd48758db4e0ba56d6eebc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08f3b7a1357641ff99786bb45c5d6e10"}},"metadata":{}},{"name":"stderr","text":"Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/49999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33d8b88437c9496cac80da1697c4f52f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c00c5270807c493eb1fa32e10b824ca5"}},"metadata":{}},{"name":"stdout","text":"Tokenized input length:  1024\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240926_101323-fde1lchm</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/kybsnyhu/huggingface/runs/fde1lchm' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/kybsnyhu/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/kybsnyhu/huggingface' target=\"_blank\">https://wandb.ai/kybsnyhu/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/kybsnyhu/huggingface/runs/fde1lchm' target=\"_blank\">https://wandb.ai/kybsnyhu/huggingface/runs/fde1lchm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='84' max='37500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   84/37500 56:29 < 429:37:40, 0.02 it/s, Epoch 0.01/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"heeee","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## try 2","metadata":{}},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom transformers import (\n    RobertaTokenizerFast,\n    RobertaForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    AutoConfig,\n)\nfrom huggingface_hub import HfFolder, notebook_login","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"notebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load dataset\ndataset = load_dataset(dataset_id)\n\n# Training and testing datasets\ntrain_dataset = dataset['train']\ntest_dataset = dataset[\"test\"].shard(num_shards=2, index=0)\n\n# Validation dataset\nval_dataset = dataset['test'].shard(num_shards=2, index=1)\n\n# Preprocessing\ntokenizer = RobertaTokenizerFast.from_pretrained(model_id)\n\n# This function tokenizes the input text using the RoBERTa tokenizer. \n# It applies padding and truncation to ensure that all sequences have the same length (256 tokens).\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=4096)\n\ntrain_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\nval_dataset = val_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))\ntest_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\nval_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\ntest_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # We will need this to directly output the class names when using the pipeline without mapping the labels later.\n# # Extract the number of classes and their names\n# num_labels = dataset['train'].features['label'].num_classes\n# class_names = dataset[\"train\"].features[\"label\"].names\n# print(f\"number of labels: {num_labels}\")\n# print(f\"the labels: {class_names}\")\n\n# # Create an id2label mapping\n# id2label = {i: label for i, label in enumerate(class_names)}\n\n# Update the model's configuration with the id2label mapping\nconfig = AutoConfig.from_pretrained(model_id)\nconfig.update({\"id2label\": id2label})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import LongformerConfig, LongformerModel\n\n\nconfiguration = LongformerConfig()\n\n\nmodel = LongformerModel(configuration)\n\n\nconfiguration = model.config","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_id = \"roberta-base\"\n# dataset_id = \"ag_news\"\n# relace the value with your model: ex <hugging-face-user>/<model-name>\nseed = 42\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed) \nrepository_id = \"/kaggle/working/\"\ntraining_args = TrainingArguments(\n    output_dir=repository_id,\n    num_train_epochs=5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    eval_strategy=\"epoch\",\n    logging_dir=f\"{repository_id}/logs\",\n    logging_strategy=\"steps\",\n    logging_steps=10,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    warmup_steps=500,\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    save_total_limit=2,\n#     report_to=\"tensorboard\",\n#     push_to_hub=True,\n#     hub_strategy=\"every_save\",\n#     hub_model_id=repository_id,\n#     hub_token=HfFolder.get_token(),\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    eval_dataset=eval_dataset,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!export CUDA_LAUNCH_BLOCKING=1\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_dataset = tokenized_dataset.map(add_labels)\neval_dataset=eval_dataset.map(add_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenized_dataset )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_dataset=Dataset.from_pandas(to_test_df)\neval_dataset=eval_dataset.map(tokenize_function, batched=True)\neval_dataset=eval_dataset.map(add_labels)\nprint(eval_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(eval_dataset[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}