{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8023365,"sourceType":"datasetVersion","datasetId":4728129},{"sourceId":171586072,"sourceType":"kernelVersion"}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install peft","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install trl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install bitsandbytes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## training code \n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    TrainingArguments,\n)\nfrom peft import LoraConfig\nfrom trl import SFTTrainer\n\n# Set CUDA devices to use (0 and 1 for two T4 GPUs)\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n\n# Model and Dataset Configuration\nmodel_name = \"deepseek-ai/deepseek-math-7b-instruct\"  # Replace with the actual model name if different\noutput_dir = \"./results\"\n\n# QLoRA parameters\nlora_r = 64  # LoRA attention dimension/rank\nlora_alpha = 16  # LoRA scaling parameter\nlora_dropout = 0.1  # LoRA dropout probability\n\n# BitsAndBytes Configuration\nuse_4bit = True\nbnb_4bit_compute_dtype = \"float16\"\nbnb_4bit_quant_type = \"nf4\"\nuse_nested_quant = False\n\n# Training Arguments\nnum_train_epochs = 5\nper_device_train_batch_size = 4  # Batch size per GPU\nper_device_eval_batch_size = 4\ngradient_accumulation_steps = 1\nlearning_rate = 2e-4\nweight_decay = 0.001\nmax_grad_norm = 0.3\noptim = \"paged_adamw_32bit\"\nlr_scheduler_type = \"cosine\"\nwarmup_ratio = 0.03\nsave_steps = 0\nlogging_steps = 50\n\n# Load Dataset\ndata = pd.read_csv('/kaggle/input/aimo-24-processor-art-of-problem-solving/problems.csv')\ndataset = Dataset.from_pandas(data)\n\n# Tokenizer and Model Loading\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\ndef preprocess_function(examples):\n    inputs = [f\"{problem} {solution} {answer}\" for problem, solution, answer in zip(examples['problem'], examples['solution'], examples['answer'])]\n    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n    return model_inputs\n\ntokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=['id', 'link', 'no', 'problem', 'solution', 'answer'])\nsplit_dataset = tokenized_dataset.train_test_split(test_size=0.1)\ntrain_dataset = split_dataset[\"train\"]\neval_dataset = split_dataset[\"test\"]\n\n# Load QLoRA config\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Set Training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    per_device_eval_batch_size=per_device_eval_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    fp16=True,  # Use fp16 for mixed precision training\n    max_grad_norm=max_grad_norm,\n    weight_decay=weight_decay,\n    lr_scheduler_type=lr_scheduler_type,\n    warmup_ratio=warmup_ratio,\n    group_by_length=True,\n    report_to=\"tensorboard\",\n)\n\n# Load model\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",  # Let Hugging Face automatically place the model on available devices\n)\n\n# SFT Trainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=512,  # Set max sequence length\n    args=training_arguments,\n    tokenizer=tokenizer,\n    packing=False,\n)\n\n# Start training\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T18:19:34.241211Z","iopub.execute_input":"2024-08-06T18:19:34.241608Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"2024-08-06 18:19:38.404389: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-06 18:19:38.404451: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-06 18:19:38.405973: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7143 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c74345850fb1401496b37d772416ee48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/22.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4e19ee35f034b0d90182d0ad226a3fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7f8592d946f47bba76fcf179db8a35a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a93f5ce09566441088a1737f8339bcef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.85G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"befdaa411f4c462f80fdeb4c899ebc25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"873bb0b0d76348c09e8cf81c2343545d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"037b044bca5d4cd2bbc1bf779f8e8bab"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\nWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='42' max='8035' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  42/8035 01:30 < 5:00:21, 0.44 it/s, Epoch 0.03/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}