{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":73231,"databundleVersionId":8365361,"sourceType":"competition"},{"sourceId":7369493,"sourceType":"datasetVersion","datasetId":4281572},{"sourceId":8012825,"sourceType":"datasetVersion","datasetId":4720595},{"sourceId":8023365,"sourceType":"datasetVersion","datasetId":4728129},{"sourceId":8052555,"sourceType":"datasetVersion","datasetId":4748944},{"sourceId":11261,"sourceType":"modelInstanceVersion","modelInstanceId":8332,"modelId":3301},{"sourceId":11264,"sourceType":"modelInstanceVersion","modelInstanceId":8318,"modelId":3301}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":724.728315,"end_time":"2024-02-29T09:37:08.760349","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-29T09:25:04.032034","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"21267b653022419eb6fc3f47aa4db8ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_926e7ccdad6440be85c76931860b744c","placeholder":"​","style":"IPY_MODEL_feef8334edb24f6da22e8bb1d8d80c67","value":"Loading checkpoint shards: 100%"}},"2144e851698b4707ad1c7fc29fe21b03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3963993becfa487c9ff725f211915e67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7a725e1b0cc4ad78a62beab5f663065","placeholder":"​","style":"IPY_MODEL_fdb32baaed7145d8a8024b615ef242ca","value":" 19/19 [10:48&lt;00:00, 33.24s/it]"}},"5882b6e860be4a0db012a64fc0704a3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21267b653022419eb6fc3f47aa4db8ed","IPY_MODEL_d91eb83d016a4381828192a98f798f9b","IPY_MODEL_3963993becfa487c9ff725f211915e67"],"layout":"IPY_MODEL_6a892a5561f742bb9db9f13859c18e90"}},"6a892a5561f742bb9db9f13859c18e90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"926e7ccdad6440be85c76931860b744c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d91eb83d016a4381828192a98f798f9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2144e851698b4707ad1c7fc29fe21b03","max":19,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0693b32889c42b18b9a3844e045d048","value":19}},"e0693b32889c42b18b9a3844e045d048":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7a725e1b0cc4ad78a62beab5f663065":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdb32baaed7145d8a8024b615ef242ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"feef8334edb24f6da22e8bb1d8d80c67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is forked from [the Notebook](https://www.kaggle.com/code/abdurrafae/improved-code-interpretation) that won the early sharing prize by @[abdurrafae](https://www.kaggle.com/abdurrafae)\n\nI tried to use this notebook to submit with the new API","metadata":{}},{"cell_type":"code","source":"import time\n\nNOTEBOOK_START_TIME = time.time()","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:57:24.085947Z","iopub.execute_input":"2024-06-20T15:57:24.086750Z","iopub.status.idle":"2024-06-20T15:57:24.090752Z","shell.execute_reply.started":"2024-06-20T15:57:24.086722Z","shell.execute_reply":"2024-06-20T15:57:24.089725Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Zero-shot MMOS-DeepSeekMath-7B with self-consistency and generated code reasoning evaluation\n\nSelf-consistency is a modification of the standard greedy decoding in reasoning pipelines via sampling several diverse answers followed by aggregation, e.g., most common answer ([SC-CoT paper](https://arxiv.org/pdf/2203.11171.pdf)).\n\nIn this kernel, we will consider MMOS-DeepSeekMath-7B RL-tuned backbone; in my experiments, this model produces more consistent code reasoning and the code block execution will allow us to decrease arithmetic hallucinations.","metadata":{}},{"cell_type":"code","source":"!pip install bitsandbytes\n!pip install accelerate\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nDEBUG = False\n\nQUANT = True\n\nif QUANT:\n    from transformers import BitsAndBytesConfig\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit = True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16,\n        bnb_4bit_use_double_quant=True,\n    )\n\nUSE_PAST_KEY = True","metadata":{"papermill":{"duration":18.075198,"end_time":"2024-02-29T09:25:25.295954","exception":false,"start_time":"2024-02-29T09:25:07.220756","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-20T15:57:29.283158Z","iopub.execute_input":"2024-06-20T15:57:29.283780Z","iopub.status.idle":"2024-06-20T15:57:31.840424Z","shell.execute_reply.started":"2024-06-20T15:57:29.283751Z","shell.execute_reply":"2024-06-20T15:57:31.839653Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"%%time\n# if QUANT:\n#     !pip install -U /kaggle/input/accelerate-wheelwhl/accelerate-0.29.1-py3-none-any.whl -qq\n#     !pip install -U /kaggle/input/bitsandbytes-0-42-0-py3-none-any-whl/bitsandbytes-0.42.0-py3-none-any.whl -qq\n\n\nimport torch\nimport gc\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\n\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    AutoConfig,\n    StoppingCriteria,\n    set_seed\n)\n\nimport transformers\nprint(f\"Transformers Version: {transformers.__version__}\")\nset_seed(42)","metadata":{"papermill":{"duration":18.075198,"end_time":"2024-02-29T09:25:25.295954","exception":false,"start_time":"2024-02-29T09:25:07.220756","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-20T15:57:31.841759Z","iopub.execute_input":"2024-06-20T15:57:31.842134Z","iopub.status.idle":"2024-06-20T15:57:35.020785Z","shell.execute_reply.started":"2024-06-20T15:57:31.842109Z","shell.execute_reply":"2024-06-20T15:57:35.019849Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Transformers Version: 4.39.3\n","output_type":"stream"},{"name":"stderr","text":"2024-06-20 15:57:32.419232: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-20 15:57:32.419296: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-20 15:57:32.420785: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 3.03 s, sys: 540 ms, total: 3.57 s\nWall time: 3.17 s\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\nimport os \n\nPRIVATE = True if os.getenv('KAGGLE_IS_COMPETITION_RERUN') else False\nPRIVATE","metadata":{"papermill":{"duration":1.224774,"end_time":"2024-02-29T09:36:31.21757","exception":false,"start_time":"2024-02-29T09:36:29.992796","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-20T15:57:35.022637Z","iopub.execute_input":"2024-06-20T15:57:35.023519Z","iopub.status.idle":"2024-06-20T15:57:35.031226Z","shell.execute_reply.started":"2024-06-20T15:57:35.023482Z","shell.execute_reply":"2024-06-20T15:57:35.030256Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}]},{"cell_type":"code","source":"def naive_parse(answer):\n    out = []\n    start = False\n    end = False\n    for l in reversed(list(answer)):\n        if l in '0123456789' and not end:\n            start = True\n            out.append(l)\n        else:\n            if start:\n                end = True\n        \n    out = reversed(out)\n    return ''.join(out)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:57:36.803927Z","iopub.execute_input":"2024-06-20T15:57:36.804629Z","iopub.status.idle":"2024-06-20T15:57:36.810204Z","shell.execute_reply.started":"2024-06-20T15:57:36.804598Z","shell.execute_reply":"2024-06-20T15:57:36.809264Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import re\nimport sys\nimport subprocess\n\ndef return_last_print(output, n):\n    lines = output.strip().split('\\n')\n    if lines:\n        return lines[n]\n    else:\n        return \"\"\n\ndef process_code(code, return_shell_output=False):\n    \n    def repl(match):\n        if \"real\" not in match.group():\n            return \"{}{}\".format(match.group()[:-1], ', real=True)')\n        else:\n            return \"{}{}\".format(match.group()[:-1], ')')\n    code = re.sub(r\"symbols\\([^)]+\\)\", repl, code)\n\n    if return_shell_output:\n        code = code.replace('\\n', '\\n    ')\n            # Add a try...except block\n        code = \"\\ntry:\\n    from sympy import *\\n{}\\nexcept Exception as e:\\n    print(e)\\n    print('FAIL')\\n\".format(code)\n    \n    if not return_shell_output:\n        print(code)\n    with open('code.py', 'w') as fout:\n        fout.write(code)\n    \n    batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n    try:\n        shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n        return_value = return_last_print(shell_output, -1)\n        print(shell_output)\n        if return_shell_output:\n            if return_value=='FAIL':\n                CODE_STATUS = False\n                return_value = return_last_print(shell_output, -2)\n                if \"not defined\" in return_value:\n                    return_value+='\\nTry checking the formatting and imports'\n            else:\n                CODE_STATUS = True\n            return return_value, CODE_STATUS  \n        code_output = round(float(eval(return_value))) % 1000\n    except Exception as e:\n        print(e,'shell_output')\n        code_output = -1\n    \n    if return_shell_output:\n        if code_output==-1:\n            CODE_STATUS = False\n        else:\n            CODE_STATUS = True\n        return code_output, CODE_STATUS  \n    \n    \n    return code_output\n\n\ndef process_text_output(output):\n    result = output    \n    try:\n        result_output = re.findall(r'\\\\boxed\\{(\\d+)\\}', result)\n\n        print('BOXED', result_output)\n        if not len(result_output):\n            result_output = naive_parse(result)\n        else:\n            result_output = result_output[-1]\n\n        print('BOXED FINAL', result_output)\n        if not len(result_output):\n            result_output = -1\n        \n        else:\n            result_output = round(float(eval(result_output))) % 1000\n    \n    except Exception as e:\n        print(e)\n        print('ERROR PARSING TEXT')\n        result_output = -1\n    \n    return result_output\n","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:57:38.419233Z","iopub.execute_input":"2024-06-20T15:57:38.420219Z","iopub.status.idle":"2024-06-20T15:57:38.436682Z","shell.execute_reply.started":"2024-06-20T15:57:38.420177Z","shell.execute_reply":"2024-06-20T15:57:38.435668Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import re\nimport math\nimport random\n\nfrom collections import defaultdict\n\nn_repetitions = 1 if PRIVATE else 1\nTOTAL_TOKENS = 2048 ","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:57:42.261724Z","iopub.execute_input":"2024-06-20T15:57:42.262372Z","iopub.status.idle":"2024-06-20T15:57:42.266976Z","shell.execute_reply.started":"2024-06-20T15:57:42.262340Z","shell.execute_reply":"2024-06-20T15:57:42.265995Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"n_repetitions","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:57:44.272150Z","iopub.execute_input":"2024-06-20T15:57:44.272508Z","iopub.status.idle":"2024-06-20T15:57:44.278508Z","shell.execute_reply.started":"2024-06-20T15:57:44.272480Z","shell.execute_reply":"2024-06-20T15:57:44.277587Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"code","source":"# !pip install -i https://pypi.org/simple/ bitsandbytes\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if True:\n\n    MODEL_PATH = \"/kaggle/input/deepseek-math\"\n    #\"/kaggle/input/gemma/transformers/7b-it/1\"\n    \n    DEEP = True\n\n    config = AutoConfig.from_pretrained(MODEL_PATH)\n    config.gradient_checkpointing = True\n\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n\n    device_map = [('model.embed_tokens', 0),\n                 ('model.layers.0', 0),\n                 ('model.layers.1', 0),\n                 ('model.layers.2', 0),\n                 ('model.layers.3', 0),\n                 ('model.layers.4', 0),\n                 ('model.layers.5', 0),\n                 ('model.layers.6', 0),\n                 ('model.layers.7', 0),\n                 ('model.layers.8', 0),\n                 ('model.layers.9', 0),\n                 ('model.layers.10', 0),\n                 ('model.layers.11', 0),\n                 ('model.layers.12', 0),\n                 ('model.layers.13', 0),\n                 ('model.layers.14', 0),\n                 ('model.layers.15', 0),\n                 ('model.layers.16', 0),\n                 ('model.layers.17', 0),\n                 ('model.layers.18', 0),\n                 ('model.layers.19', 0),\n                 ('model.layers.20', 0),\n                 ('model.layers.21', 0),\n                 ('model.layers.22', 1),\n                 ('model.layers.23', 1),\n                 ('model.layers.24', 1),\n                 ('model.layers.25', 1),\n                 ('model.layers.26', 1),\n                 ('model.layers.27', 1),\n                 ('model.layers.28', 1),\n                 ('model.layers.29', 1),\n                 ('model.norm', 1),\n                 ('lm_head', 1)]\n\n    device_map = {ii:jj for (ii,jj) in device_map}\n\n    if QUANT:\n        from transformers import BitsAndBytesConfig\n        quantization_config = BitsAndBytesConfig(\n            load_in_4bit = True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16,\n            bnb_4bit_use_double_quant=True,\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            MODEL_PATH,\n            device_map=\"sequential\",\n            torch_dtype=\"auto\",\n            trust_remote_code=True, \n            quantization_config=quantization_config,\n            config=config\n        )\n    else:  \n        model = AutoModelForCausalLM.from_pretrained(\n            MODEL_PATH,\n            device_map=device_map,\n            torch_dtype=\"auto\",\n            trust_remote_code=True,\n            quantization_config=quantization_config,\n            config=config\n        )\n    \n    pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype='auto',\n    device_map=device_map,\n)\n    from transformers import StoppingCriteriaList\n\n    class StoppingCriteriaSub(StoppingCriteria):\n        def __init__(self, stops = [], encounters=1):\n            super().__init__()\n            self.stops = [stop.to(\"cuda\") for stop in stops]\n\n        def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n            for stop in self.stops:\n                last_token = input_ids[0][-len(stop):]\n                if torch.all(torch.eq(stop,last_token)):\n                    return True\n            return False\n\n\n    stop_words = [\"```output\", \"```python\", \"```\\nOutput\" , \")\\n```\" , \"``````output\"] #,  \n    stop_words_ids = [tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze() for stop_word in stop_words]\n    stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n    \n    model.dtype, model.hf_device_map","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:57:46.283080Z","iopub.execute_input":"2024-06-20T15:57:46.283603Z","iopub.status.idle":"2024-06-20T15:58:44.175618Z","shell.execute_reply.started":"2024-06-20T15:57:46.283569Z","shell.execute_reply":"2024-06-20T15:58:44.174770Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b528c63b8fc849edb424fde3e760ccd2"}},"metadata":{}}]},{"cell_type":"markdown","source":"## saving the quantized model\n","metadata":{}},{"cell_type":"code","source":"save_directory=\"/kaggle/working/deepseekmath\"\nmodel.save_pretrained(save_directory)\n\n# Optionally, save the tokenizer as well if you have one\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:58:46.441906Z","iopub.execute_input":"2024-06-20T15:58:46.442391Z","iopub.status.idle":"2024-06-20T15:58:59.383077Z","shell.execute_reply.started":"2024-06-20T15:58:46.442355Z","shell.execute_reply":"2024-06-20T15:58:59.382069Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"code = \"\"\"Below is a math problem you are to solve (positive numerical answer):\n\\\"{}\\\"\nTo accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear so even an idiot can follow your instructions, and remember, your final answer should be positive integer, not an algebraic expression!\nWrite the entire script covering all the steps (use comments and document it well) and print the result. After solving the problem, output the final numerical answer within \\\\boxed{}.\n\nApproach:\"\"\"\n\n\ncot = \"\"\"Below is a math problem you are to solve (positive numerical answer!):\n\\\"{}\\\"\nAnalyze this problem and think step by step to come to a solution with programs. After solving the problem, output the final numerical answer within \\\\boxed{}.\\n\\n\"\"\"\n\npromplt_options = [code,cot]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom collections import defaultdict\nfrom collections import Counter\n\nfrom numpy.random import choice\nimport numpy as np\n\ntool_instruction = '\\n\\nPlease integrate natural language reasoning with programs to solve the above problem, and put your final numerical answer within \\\\boxed{}.\\nNote that the intermediary calculations may be real numbers, but the final numercal answer would always be an integer.'\n\n\n#tool_instruction = \" The answer should be given as a non-negative modulo 1000.\"\n#tool_instruction += '\\nPlease integrate natural language reasoning with programs to solve the problem above, and put your final answer within \\\\boxed{}.'\n\ntemperature = 0.9\ntop_p = 3.0\n\ntemperature_coding = 0.9\ntop_p_coding = 3.0\n\n   \ntotal_results = {}\ntotal_answers = {}\nbest_stats = {}\ntotal_outputs = {}\nquestion_type_counts = {}\nstarting_counts = (2,3)","metadata":{"papermill":{"duration":34.259365,"end_time":"2024-02-29T09:37:05.548829","exception":false,"start_time":"2024-02-29T09:36:31.289464","status":"completed"},"tags":[],"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making a function for prediction!","metadata":{}},{"cell_type":"code","source":"def predict(problem):\n    \n    temperature = 0.9\n    top_p = 3.0\n\n    temperature_coding = 0.9\n    top_p_coding = 3.0\n\n   \n    total_results = {}\n    total_answers = {}\n    best_stats = {}\n    total_outputs = {}\n    question_type_counts = {}\n    starting_counts = (2,3)\n    i = 0\n    \n    global n_repetitions,TOTAL_TOKENS,model,tokenizer,USE_PAST_KEY,NOTEBOOK_START_TIME,promplt_options,code,cot\n    \n    \n    if time.time()-NOTEBOOK_START_TIME>=31500:\n        return 0\n    \n    for jj in tqdm(range(n_repetitions)):\n        best, best_count = best_stats.get(i,(-1,-1))\n        if best_count>np.sqrt(jj):\n            print(\"SKIPPING CAUSE ALREADY FOUND BEST\")\n            continue\n\n        outputs = total_outputs.get(i,[])\n        text_answers, code_answers = question_type_counts.get(i,starting_counts)\n        results = total_results.get(i,[])\n        answers = total_answers.get(i,[])  \n        \n        for _ in range(5):\n            torch.cuda.empty_cache()\n            gc.collect()\n            time.sleep(0.2)\n        \n        try:\n            ALREADY_GEN = 0\n            code_error = None\n            code_error_count = 0\n            code_output = -1\n            #initail_message = problem  + tool_instruction \n            counts = np.array([text_answers,code_answers])\n\n            draw = choice(promplt_options, 1,\n                          p=counts/counts.sum())\n\n            initail_message = draw[0].format(problem,\"{}\")            \n            prompt = f\"User: {initail_message}\"\n\n            current_printed = len(prompt)\n            print(f\"{jj}_{prompt}\\n\")\n\n            model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n            input_len = len(model_inputs['input_ids'][0])\n\n            generation_output = model.generate(**model_inputs, \n                                               max_new_tokens=TOTAL_TOKENS-ALREADY_GEN,\n                                               return_dict_in_generate=USE_PAST_KEY,\n                                               do_sample = True,\n                                               temperature = temperature,\n                                               top_p = top_p,\n                                               num_return_sequences=1, stopping_criteria = stopping_criteria)\n\n            if USE_PAST_KEY:\n                output_ids = generation_output.sequences[0]\n            else:\n                output_ids = generation_output[0]\n            decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n            print(f\"{decoded_output[current_printed:]}\\n\")\n            current_printed += len(decoded_output[current_printed:])\n            cummulative_code = \"\"\n            \n            stop_word_cond = False\n            for stop_word in stop_words:\n                stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n                \n                \n            while (stop_word_cond) and (ALREADY_GEN<(TOTAL_TOKENS)):\n\n                if (decoded_output[-len(\"```python\"):]==\"```python\"):\n                    temperature_inner=temperature_coding\n                    top_p_inner = top_p_coding\n                    prompt = decoded_output\n                else:\n                    temperature_inner=temperature\n                    top_p_inner = top_p\n                    try:\n                        if (decoded_output[-len(\"``````output\"):]==\"``````output\"):\n                            code_text = decoded_output.split('```python')[-1].split(\"``````\")[0]\n                        else:\n                            code_text = decoded_output.split('```python')[-1].split(\"```\")[0]\n                        \n\n                        cummulative_code+=code_text\n                        code_output, CODE_STATUS = process_code(cummulative_code, return_shell_output=True)\n                        print('CODE RESULTS', code_output)\n\n                        if code_error==code_output:\n                            code_error_count+=1\n                        else:\n                            code_error=code_output\n                            code_error_count = 0\n\n                        if not CODE_STATUS:\n                            cummulative_code = cummulative_code[:-len(code_text)]\n\n                            if code_error_count>=1:\n                                print(\"REPEATED ERRORS\")\n                                break\n\n                    except Exception as e:\n                        print(e)\n                        print('ERROR PARSING CODE')\n                        code_output = -1\n\n                    if code_output!=-1:\n                        if (decoded_output[-len(\")\\n```\"):]==\")\\n```\"):\n                            prompt = decoded_output+'```output\\n'+str(code_output)+'\\n```\\n'\n                        else:\n                            prompt = decoded_output+'\\n'+str(code_output)+'\\n```\\n'\n                    else:\n                        prompt = decoded_output\n                        cummulative_code=\"\"\n                model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n                ALREADY_GEN =  len(model_inputs['input_ids'][0])-input_len\n\n                if USE_PAST_KEY:\n                    old_values = generation_output.past_key_values\n                else:\n                    old_values = None\n\n                generation_output = model.generate(**model_inputs, \n                                                   max_new_tokens=TOTAL_TOKENS-ALREADY_GEN, \n                                                   return_dict_in_generate=USE_PAST_KEY,\n                                                   past_key_values=old_values,\n                                                   do_sample = True,\n                                                   temperature = temperature_inner,\n                                                   top_p = top_p_inner,\n                                                   num_return_sequences=1, stopping_criteria = stopping_criteria)\n                if USE_PAST_KEY:\n                    output_ids = generation_output.sequences[0]\n                else:\n                    output_ids = generation_output[0]\n                decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n                print(f\"\\nINTERMEDIATE OUT :\\n{decoded_output[current_printed:]}\\n\")\n                current_printed+=len(decoded_output[current_printed:])\n                \n                stop_word_cond = False\n                for stop_word in stop_words:\n                    stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n            if USE_PAST_KEY:\n                output_ids = generation_output.sequences[0]\n            else:\n                output_ids = generation_output[0]\n\n            raw_output = tokenizer.decode(output_ids[input_len:], skip_special_tokens=True)\n            #print(f\"\\n\\nOutput :\\n{raw_output}\\n\")                            \n            result_output = process_text_output(raw_output)\n            \n            try:\n                code_output = round(float(eval(code_output))) % 1000\n            except Exception as e:\n                print(e,'final_eval')\n                code_output = -1\n        except Exception as e:\n            print(e,\"5\")\n            result_output, code_output = -1, -1\n\n        if code_output!=-1:\n            outputs.append(code_output)\n            code_answers+=1\n\n        if result_output!=-1:\n            outputs.append(result_output)\n            text_answers+=1\n\n        if len(outputs) > 0:\n            occurances = Counter(outputs).most_common()\n            print(occurances)\n            if occurances[0][1] > best_count:\n                print(\"GOOD ANSWER UPDATED!\")\n                best = occurances[0][0]\n                best_count = occurances[0][1]\n            if occurances[0][1] > 5:\n                print(\"ANSWER FOUND!\")\n                break\n\n        results.append(result_output)\n        answers.append(code_output)\n        \n        best_stats[i] = (best, best_count) \n        question_type_counts[i] = (text_answers, code_answers)\n        total_outputs[i] = outputs\n        \n        total_results[i] = results\n        total_answers[i] = answers\n\n        print(\"code_answers\",code_answers-starting_counts[1],\"text_answers\",text_answers-starting_counts[0])\n    return best_stats[0][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir deepseekmath\n%pwd /kaggle/working/deepseekmath","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_directory=\"/kaggle/working/deepseekmath\"\nmodel.save_pretrained(save_directory)\n\n# Optionally, save the tokenizer as well if you have one\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import huggingface_hub\nhuggingface_hub.login(token=\"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\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo apt-get install git-lfs\n!git-lfs install","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.chdir(\"/kaggle/working/deepseekmath\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git init\n!git add . && git commit -m \"Update from $USER\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git config --global user.email \"apratham489@gmail.com\"\n!git config --global user.name \"prathamadh\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git remote add origin https://huggingface.co/Pra-tham/deepseekmath","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git checkout -b main","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git push -f --set-upstream origin main ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_path = \"deepseekmath\"\npeft_model_id = \"Pra-tham/\" + save_path\n\nprint(\"training_finished saving model and tokenizer\")\n\n# model = model.merge\nmodel.push_to_hub(peft_model_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Prediction on the train set to see if the function works!**","metadata":{}},{"cell_type":"code","source":"if not PRIVATE:\n    df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n    df['model_answer'] = df['problem'].apply(lambda x:predict(x))\n    df['match'] = df.answer == df.model_answer\n    print(f'{df.match.sum()} matches in {len(df)} examples')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission with the new API","metadata":{}},{"cell_type":"code","source":"import aimo\n\nenv = aimo.make_env()\niter_test = env.iter_test()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    for test, sample_submission in iter_test:\n        sample_submission['answer'] = predict(test['problem'].values[0])\n        env.predict(sample_submission)\n        print(test)\n        print(sample_submission)\n        \nelse:\n    for test, sample_submission in iter_test:\n        sample_submission['answer'] = predict(test['problem'].values[0])\n        env.predict(sample_submission)\n        print(test)\n        print(sample_submission)\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('code.py', 'w') as fout:\n    fout.write(\"print('done')\")\n\nbatcmd = 'timeout 7 ' + sys.executable + ' code.py'\ntry:\n    shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n    print(shell_output)\nexcept:\n    pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_model_summary(model):\n    # Print model architecture\n    print(model)\n\n    # Calculate and print the number of parameters in each layer\n    total_params = sum(p.numel() for p in model.parameters())\n    print(f\"Total trainable parameters: {total_params}\")\n\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            num_params = param.numel()\n            print(f\"{name}: {num_params} parameters\")\n\nprint_model_summary(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}